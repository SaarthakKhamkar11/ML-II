{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name : Saarthak Khamkar   \n",
        "Roll No. : D088   \n",
        "SAP ID : 60009230057"
      ],
      "metadata": {
        "id": "_7tvvUIK5YeU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PL1rPwaWAEuI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.models as models\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler  # mixed precision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "91Ktt8hoBjS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simclr_transform = T.Compose([\n",
        "    T.RandomResizedCrop(32),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomApply([T.ColorJitter(0.4,0.4,0.4,0.1)], p=0.8),\n",
        "    T.RandomGrayscale(p=0.2),\n",
        "    T.GaussianBlur(kernel_size=3),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=simclr_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "rDZI72wYBYeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2cfd6fe-974a-44ab-bf40-32ed9e2885ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:10<00:00, 16.1MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Network"
      ],
      "metadata": {
        "id": "67IXZmZkBqrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.backbone = nn.Sequential(*list(base_model.children())[:-1])\n",
        "        self.fc = nn.Linear(base_model.fc.in_features, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return F.normalize(x, dim=1)\n"
      ],
      "metadata": {
        "id": "b_vBlCfOBopn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projection Head"
      ],
      "metadata": {
        "id": "CYxAsgCUBv6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, in_dim=128, out_dim=128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dzgbDbFOBtiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimCLR Model"
      ],
      "metadata": {
        "id": "XziSljIcB0Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(base_model)\n",
        "        self.projection_head = ProjectionHead()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.projection_head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "aVhYXvz9BykL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NT-Xent Loss"
      ],
      "metadata": {
        "id": "1ElOjxBKB30a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        z = torch.cat([z_i, z_j], dim=0)  # 2N x D\n",
        "        sim = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2) / self.temperature\n",
        "        mask = torch.eye(sim.size(0), device=z.device).bool()\n",
        "        sim = sim.masked_fill(mask, -9e15)  # mask self-similarity\n",
        "        labels = torch.arange(z_i.size(0), device=z.device)\n",
        "        labels = torch.cat([labels, labels], dim=0)\n",
        "        loss = F.cross_entropy(sim, labels)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "5GEKKQffB2Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "m2O-lPsrCAPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    scaler = GradScaler()  # mixed precision\n",
        "\n",
        "    for x, _ in tqdm(loader):\n",
        "        x = x.to(device)\n",
        "        # Create two views for contrastive learning\n",
        "        x_i = x\n",
        "        x_j = x.clone()  # You can also apply a separate augmentation here\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():  # mixed precision\n",
        "            z_i, z_j = model(x_i), model(x_j)\n",
        "            loss = criterion(z_i, z_j)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ],
      "metadata": {
        "id": "c3la4N8JB6Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "GQGI0P32CEVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model = models.resnet18(pretrained=True)  # use pretrained for faster convergence\n",
        "model = SimCLR(base_model).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = NTXentLoss().to(device)\n",
        "\n",
        "for epoch in range(1):\n",
        "    loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7E4skE4hCCsS",
        "outputId": "5d07f147-74a0-4485-e8c9-bdfc377664c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 167MB/s]\n",
            "/tmp/ipython-input-973147274.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # mixed precision\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-973147274.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # mixed precision\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "100%|██████████| 391/391 [26:36<00:00,  4.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4500000175654456.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Link : https://colab.research.google.com/drive/13bivmbUU0z0gGcpg-vnuLDBTfEdAEtkj?usp=sharing"
      ],
      "metadata": {
        "id": "cXb0HIoK5hjy"
      }
    }
  ]
}